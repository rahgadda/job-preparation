{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVOjogJoePSEK0hLmPzOK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahgadda/knowledge-base/blob/main/BERT_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ph3BC3S04fYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0347c2b-4eb1-4fa3-907f-e3dc52bc25b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.32.2-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.56.tar.gz (36.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Collecting packaging<24,>=16.8 (from streamlit)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.10.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.27-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.56-cp310-cp310-manylinux_2_35_x86_64.whl size=2835005 sha256=050996fabd6ad9f5e732b2920ee47b098c998e0492d31b734f52e9ef9fa9d532\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/09/9d/c413053f6258cb2546cc792418c595e276f9efd5db31a80377\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: faiss-gpu, watchdog, smmap, pypdf, packaging, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, jsonpointer, diskcache, typing-inspect, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, marshmallow, llama-cpp-python, jsonpatch, gitdb, nvidia-cusolver-cu12, langsmith, gitpython, dataclasses-json, langchain-core, streamlit, sentence-transformers, langchain-text-splitters, langchain_community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 diskcache-5.6.3 faiss-gpu-1.7.2 gitdb-4.0.11 gitpython-3.1.42 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langchain_community-0.0.28 langsmith-0.1.27 llama-cpp-python-0.2.56 marshmallow-3.21.1 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 orjson-3.9.15 packaging-23.2 pydeck-0.8.1b0 pypdf-4.1.0 sentence-transformers-2.5.1 smmap-5.0.1 streamlit-1.32.2 typing-inspect-0.9.0 watchdog-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1\n",
        "!pip install streamlit requests pypdf faiss-gpu llama-cpp-python sentence-transformers torch langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Upload pdf file into 'pdf-data' folder if it does not exist\n",
        "def fn_upload_pdf(mv_pdf_input_file, mv_processing_message):\n",
        "    \"\"\"Upload pdf file into 'pdf-data' folder if it does not exist\"\"\"\n",
        "\n",
        "    lv_file_name = mv_pdf_input_file.name\n",
        "\n",
        "    if not os.path.exists(\"pdf-data\"):\n",
        "        os.makedirs(\"pdf-data\")\n",
        "\n",
        "    lv_temp_file_path = os.path.join(\"pdf-data\",lv_file_name)\n",
        "\n",
        "    if os.path.exists(lv_temp_file_path):\n",
        "        print(\"File already available\")\n",
        "        fn_display_user_messages(\"File already available\",\"Warning\", mv_processing_message)\n",
        "    else:\n",
        "        with open(lv_temp_file_path,\"wb\") as lv_file:\n",
        "            lv_file.write(mv_pdf_input_file.getbuffer())\n",
        "\n",
        "        print(\"Step1: PDF uploaded successfully at -> \" + lv_temp_file_path)\n",
        "        fn_display_user_messages(\"Step1: PDF uploaded successfully at -> \" + lv_temp_file_path, \"Info\", mv_processing_message)\n",
        "\n",
        "# Create Vector DB of uploaded PDF\n",
        "def fn_create_vector_db(mv_pdf_input_file, mv_processing_message):\n",
        "    \"\"\"Create Vector DB of uploaded PDF\"\"\"\n",
        "\n",
        "    lv_file_name = mv_pdf_input_file.name[:-4] + \".vectorstore\"\n",
        "\n",
        "    if not os.path.exists(os.path.join(\"vectordb\",\"fiaas\")):\n",
        "        os.makedirs(os.path.join(\"vectordb\",\"fiaas\"))\n",
        "\n",
        "    lv_temp_file_path = os.path.join(os.path.join(\"vectordb\",\"fiaas\"),lv_file_name)\n",
        "    lv_embeddings = HuggingFaceEmbeddings(\n",
        "                                            model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                                            model_kwargs={'device': 'cpu'}\n",
        "                                        )\n",
        "\n",
        "    if os.path.exists(lv_temp_file_path):\n",
        "        print(\"VectorDB already available for uploaded file\")\n",
        "        fn_display_user_messages(\"VectorDB already available for uploaded file\",\"Warning\", mv_processing_message)\n",
        "\n",
        "        lv_vector_store = FAISS.load_local(lv_temp_file_path, lv_embeddings,allow_dangerous_deserialization=True)\n",
        "        return lv_vector_store\n",
        "    else:\n",
        "        lv_temp_pdf_file_path = os.path.join(\"pdf-data\",mv_pdf_input_file.name)\n",
        "\n",
        "        # -- Loading PDF Data\n",
        "        lv_pdf_loader = PyPDFLoader(lv_temp_pdf_file_path)\n",
        "        lv_pdf_content = lv_pdf_loader.load()\n",
        "\n",
        "        # -- Define patterns with flexibility\n",
        "        pattern1 = r\"(\\w+)-\\n(\\w+)\"  # Match hyphenated words separated by a line break\n",
        "        pattern2 = r\"(?<!\\n\\s)\\n(?!\\s\\n)\"  # Match line breaks not surrounded by whitespace\n",
        "        pattern3 = r\"\\n\\s*\\n\"  # Match multiple line breaks with optional whitespace\n",
        "\n",
        "        lv_pdf_formatted_content = []\n",
        "        for lv_page in lv_pdf_content:\n",
        "            # -- Apply substitutions with flexibility\n",
        "            lv_pdf_page_content = re.sub(pattern1, r\"\\1\\2\", lv_page.page_content)\n",
        "            lv_pdf_page_content = re.sub(pattern2, \" \", lv_pdf_page_content.strip())\n",
        "            lv_pdf_page_content = re.sub(pattern3, \" \", lv_pdf_page_content)\n",
        "            lv_pdf_page_content = re.sub(\"\\n\", \" \", lv_pdf_page_content)\n",
        "\n",
        "            lv_pdf_formatted_content.append(Document( page_content= lv_pdf_page_content,\n",
        "                                                      metadata= lv_page.metadata)\n",
        "                                           )\n",
        "\n",
        "            print(\"Page Details of \"+str(lv_page.metadata)+\" is - \"+lv_pdf_page_content)\n",
        "\n",
        "        print(\"Step2: PDF content extracted\")\n",
        "        fn_display_user_messages(\"Step2: PDF content extracted\", \"Info\", mv_processing_message)\n",
        "\n",
        "        # -- Chunking PDF Data\n",
        "        lv_text_splitter = CharacterTextSplitter(\n",
        "                                                    separator=\"\\n\",\n",
        "                                                    chunk_size=300,\n",
        "                                                    chunk_overlap=30,\n",
        "                                                    length_function=len\n",
        "                                                )\n",
        "        lv_pdf_chunk_documents = lv_text_splitter.split_documents(lv_pdf_formatted_content)\n",
        "        print(\"Step3: PDF content chucked and document object created\")\n",
        "        fn_display_user_messages(\"Step3: PDF content chucked and document object created\", \"Info\", mv_processing_message)\n",
        "\n",
        "        # -- Creating FIASS Vector Store\n",
        "        lv_vector_store = FAISS.from_documents(lv_pdf_chunk_documents, lv_embeddings)\n",
        "        print(\"Step4: Vector store created\")\n",
        "        fn_display_user_messages(\"Step4: Vector store created\", \"Info\", mv_processing_message)\n",
        "        lv_vector_store.save_local(lv_temp_file_path)\n",
        "\n",
        "        return lv_vector_store\n",
        "\n",
        "# Display user Error, Warning or Success Message\n",
        "def fn_display_user_messages(lv_text, lv_type, mv_processing_message):\n",
        "    \"\"\"Display user Info, Error, Warning or Success Message\"\"\"\n",
        "\n",
        "    if lv_type == \"Success\":\n",
        "        with mv_processing_message.container():\n",
        "            st.success(lv_text)\n",
        "    elif lv_type == \"Error\":\n",
        "        with mv_processing_message.container():\n",
        "            st.error(lv_text)\n",
        "    elif lv_type == \"Warning\":\n",
        "        with mv_processing_message.container():\n",
        "            st.warning(lv_text)\n",
        "    else:\n",
        "        with mv_processing_message.container():\n",
        "            st.info(lv_text)\n",
        "\n",
        "# Download TheBloke Models\n",
        "def fn_download_llm_models(mv_selected_model, mv_processing_message):\n",
        "    \"\"\"Download TheBloke Models\"\"\"\n",
        "\n",
        "    lv_download_url = \"\"\n",
        "\n",
        "    print(\"Downloading TheBloke of \"+mv_selected_model)\n",
        "    fn_display_user_messages(\"Downloading TheBloke of \"+mv_selected_model, \"Info\", mv_processing_message)\n",
        "\n",
        "    if mv_selected_model == 'microsoft/phi-2':\n",
        "        lv_download_url = \"https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf\"\n",
        "    elif mv_selected_model == 'google/gemma-2b':\n",
        "        lv_download_url = \"https://huggingface.co/MaziyarPanahi/gemma-2b-it-GGUF/resolve/main/gemma-2b-it.Q2_K.gguf\"\n",
        "    elif mv_selected_model == 'mistralai/Mistral-7B-Instruct-v0.2':\n",
        "        lv_download_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q2_K.gguf\"\n",
        "\n",
        "    if not os.path.exists(\"model\"):\n",
        "        os.makedirs(\"model\")\n",
        "\n",
        "    lv_filename = os.path.basename(lv_download_url)\n",
        "    lv_temp_file_path = os.path.join(\"model\",lv_filename)\n",
        "\n",
        "    if os.path.exists(lv_temp_file_path):\n",
        "        print(\"Model already available\")\n",
        "        fn_display_user_messages(\"Model already available\",\"Warning\", mv_processing_message)\n",
        "    else:\n",
        "        lv_response = requests.get(lv_download_url, stream=True)\n",
        "        if lv_response.status_code == 200:\n",
        "            with open(lv_temp_file_path, 'wb') as f:\n",
        "                for chunk in lv_response.iter_content(chunk_size=1024):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "\n",
        "            print(\"Download completed\")\n",
        "            fn_display_user_messages(\"Model download completed\",\"Info\", mv_processing_message)\n",
        "        else:\n",
        "            print(f\"Model download completed {response.status_code}\")\n",
        "            fn_display_user_messages(f\"Model download completed {response.status_code}\",\"Error\", mv_processing_message)\n",
        "\n",
        "# Function return QA Response using Vector Store\n",
        "def fn_generate_QnA_response(mv_selected_model, mv_user_question, lv_vector_store, mv_processing_message):\n",
        "    \"\"\"Returns QA Response using Vector Store\"\"\"\n",
        "\n",
        "    lv_model_path = \"\"\n",
        "    lv_model_type = \"\"\n",
        "    lv_template   = \"\"\"Instruction:\n",
        "                    You are an AI assistant for answering questions about the provided context.\n",
        "                    You are given the following extracted parts of a long document and a question. Provide a detailed answer.\n",
        "                    If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
        "                    =======\n",
        "                    {context}\n",
        "                    =======\n",
        "                    Question: {question}\n",
        "                    Output:\\n\"\"\"\n",
        "    lv_qa_prompt = PromptTemplate(\n",
        "                                template=lv_template,\n",
        "                                input_variables=[\"question\", \"context\"]\n",
        "                              )\n",
        "\n",
        "    if mv_selected_model == 'microsoft/phi-2':\n",
        "        lv_model_path = \"model/phi-2.Q2_K.gguf\"\n",
        "        lv_model_type = \"pi\"\n",
        "    elif mv_selected_model == 'google/gemma-2b':\n",
        "        lv_model_path = \"model/gemma-2b-it.Q2_K.gguf\"\n",
        "        lv_model_type = \"gemma\"\n",
        "    elif mv_selected_model == 'mistralai/Mistral-7B-Instruct-v0.2':\n",
        "        lv_model_path = \"model/mistral-7b-instruct-v0.2.Q2_K.gguf\"\n",
        "        lv_model_type = \"mistral\"\n",
        "\n",
        "    print(\"Step4: Generating LLM response\")\n",
        "    fn_display_user_messages(\"Step4: Generating LLM response\",\"Info\", mv_processing_message)\n",
        "\n",
        "    lv_model = LlamaCpp(\n",
        "                            model_path=lv_model_path,\n",
        "                            temperature=0.00,\n",
        "                            max_tokens=2048,\n",
        "                            top_p=1,\n",
        "                            n_ctx=2048,\n",
        "                            n_gpu_layers=64,\n",
        "                            verbose=False\n",
        "                       )\n",
        "    lv_vector_search_result = lv_vector_store.similarity_search(mv_user_question, k=2)\n",
        "    # print(\"Vector Search Result - \")\n",
        "    # print(lv_vector_search_result)\n",
        "\n",
        "    # -- Creating formatted document result\n",
        "    lv_document_context = \"\"\n",
        "    lv_count = 0\n",
        "    for lv_result in lv_vector_search_result:\n",
        "        print(\"Concatenating Result of page - \" + str(lv_count) + \" with content of document page no - \"+str(lv_result.metadata[\"page\"]))\n",
        "        lv_document_context += lv_result.page_content\n",
        "        lv_count += 1\n",
        "\n",
        "    # print(\"Formatted Document Search Result - \")\n",
        "    # print(lv_document_context)\n",
        "\n",
        "    lv_qa_formatted_prompt = lv_qa_prompt.format(\n",
        "                                            question=mv_user_question,\n",
        "                                            context=lv_document_context\n",
        "                                         )\n",
        "    print(\"Formatted Prompt - \" + lv_qa_formatted_prompt)\n",
        "\n",
        "    lv_llm_response = lv_model(lv_qa_formatted_prompt)\n",
        "    # print(\"LLM Response\" +lv_llm_response)\n",
        "\n",
        "    print(\"Step5: LLM response generated\")\n",
        "    fn_display_user_messages(\"Step5: LLM response generated\",\"Info\", mv_processing_message)\n",
        "\n",
        "    return lv_llm_response\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "\n",
        "    # -- Streamlit Settings\n",
        "    st.set_page_config(layout='wide')\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    col2.title(\"Chat with PDF\")\n",
        "    st.text(\"\")\n",
        "\n",
        "    # -- Initialize chat history\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    # -- Display Supported Models\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    mv_selected_model = col3.selectbox('Select Model',\n",
        "                                        [\n",
        "                                            'microsoft/phi-2',\n",
        "                                            'google/gemma-2b',\n",
        "                                            'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "                                        ]\n",
        "                                      )\n",
        "\n",
        "    # -- Display Supported Vector Stores\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    mv_selected_vector_db = col3.selectbox('Select Vector DB', ['FAISS'])\n",
        "    st.text(\"\")\n",
        "\n",
        "    # -- Reading PDF File\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    mv_pdf_input_file = col2.file_uploader(\"Choose a PDF file:\", type=[\"pdf\"])\n",
        "\n",
        "    # -- Display Processing Details\n",
        "    st.text(\"\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    mv_processing_message = col2.empty()\n",
        "    st.text(\"\")\n",
        "\n",
        "    # -- Downloading Model Files\n",
        "    fn_download_llm_models(mv_selected_model, mv_processing_message)\n",
        "\n",
        "    # -- Processing PDF\n",
        "    if (mv_pdf_input_file is not None):\n",
        "\n",
        "        # -- Upload PDF\n",
        "        fn_upload_pdf(mv_pdf_input_file, mv_processing_message)\n",
        "\n",
        "        # -- Create Vector Index\n",
        "        lv_vector_store = fn_create_vector_db(mv_pdf_input_file, mv_processing_message)\n",
        "\n",
        "        # -- Perform RAG\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        st.text(\"\")\n",
        "        lv_chat_history = col2.chat_message\n",
        "        st.text(\"\")\n",
        "\n",
        "        if mv_user_question := col2.chat_input(\"Chat on PDF Data\"):\n",
        "           # -- Add user message to chat history\n",
        "           st.session_state.messages.append({\"role\": \"user\", \"content\": mv_user_question})\n",
        "\n",
        "           # -- Generating LLM response\n",
        "           lv_response = fn_generate_QnA_response(mv_selected_model, mv_user_question, lv_vector_store, mv_processing_message)\n",
        "\n",
        "           # -- Adding assistant response to chat history\n",
        "           st.session_state.messages.append({\"role\": \"assistant\", \"content\": lv_response})\n",
        "\n",
        "           # -- Display chat messages from history on app rerun\n",
        "           for message in st.session_state.messages:\n",
        "               with lv_chat_history(message[\"role\"]):\n",
        "                   st.markdown(message[\"content\"])\n",
        "\n",
        "        # -- Validate Data\n",
        "\n",
        "        # -- Get Web Response\n",
        "\n",
        "# Calling Main Function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfuMxoWde6NC",
        "outputId": "2f73d575-b62b-4e21-acca-75a3ccd41b4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!export COMMANDLINE_ARGS=\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnBvIwdNfp1E",
        "outputId": "d7975417-31e2-4eb7-dfbc-6b82f4482cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.799s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.546s\n",
            "your url is: https://smart-teeth-lick.loca.lt\n"
          ]
        }
      ]
    }
  ]
}